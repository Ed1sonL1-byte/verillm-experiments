# Running VeriLLM Experiment 1 on 8Ã—A100 GPUs

## ğŸ¯ Quick Start (TL;DR)

```bash
# On your A100 server
cd /path/to/verillm-experiments

# Make script executable
chmod +x run_exp1_a100.sh

# Run experiment
./run_exp1_a100.sh
```

**Expected time**: ~30-45 minutes total (3 prompts Ã— ~10-15 min each)

---

## ğŸ“‹ Detailed Step-by-Step Guide

### Step 1: SSH to A100 Server

```bash
ssh your-username@your-a100-server

# Navigate to project
cd /path/to/verillm-experiments
```

### Step 2: Verify GPU Access

```bash
nvidia-smi

# Expected output: Should show 8Ã— A100 GPUs
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 525.xx.xx    Driver Version: 525.xx.xx    CUDA Version: 12.0   |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |                               |                      |               MIG M. |
# |===============================+======================+======================|
# |   0  NVIDIA A100-SXM...  On   | 00000000:00:04.0 Off |                    0 |
# | N/A   30C    P0    56W / 400W |      0MiB / 81920MiB |      0%      Default |
# ...
# |   7  NVIDIA A100-SXM...  On   | 00000000:00:0B.0 Off |                    0 |
```

### Step 3: Activate Environment

```bash
# If venv doesn't exist, create it:
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Otherwise, just activate:
source venv/bin/activate

# Verify CUDA in PyTorch
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
# Expected: CUDA: True, GPUs: 8
```

### Step 4: Configure Model Download (First Time Only)

å®éªŒä¼šè‡ªåŠ¨ä»HuggingFaceä¸‹è½½æ¨¡å‹ã€‚æœ‰ä¸¤ç§æ–¹å¼ï¼š

**Option A: è‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰**
```bash
# æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° ~/.cache/huggingface/
# ç¬¬ä¸€æ¬¡è¿è¡Œä¼šæ¯”è¾ƒæ…¢ï¼ˆ~20GBä¸‹è½½ï¼‰
# åç»­è¿è¡Œä¼šä½¿ç”¨ç¼“å­˜
```

**Option B: é¢„å…ˆä¸‹è½½åˆ°æœ¬åœ°**
```bash
# ç¼–è¾‘ configs/models.yamlï¼Œç¡®ä¿ local_path æ­£ç¡®
# å¦‚æœå·²æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œå°†å®ƒä»¬æ”¾åœ¨ ./models/ ç›®å½•

mkdir -p models/qwen2.5-7b

# ä»HuggingFaceä¸‹è½½ï¼ˆéœ€è¦git-lfsï¼‰
git lfs install
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct models/qwen2.5-7b
```

### Step 5: Run Experiment

**æ–¹å¼1ï¼šä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰**
```bash
chmod +x run_exp1_a100.sh
./run_exp1_a100.sh
```

**æ–¹å¼2ï¼šç›´æ¥è¿è¡ŒPython**
```bash
cd experiments
python exp1_homogeneous.py
```

**æ–¹å¼3ï¼šä½¿ç”¨nohupåå°è¿è¡Œ**
```bash
# å¦‚æœå®éªŒæ—¶é—´é•¿ï¼Œä½¿ç”¨nohupé˜²æ­¢SSHæ–­å¼€å¯¼è‡´ä¸­æ–­
nohup ./run_exp1_a100.sh > exp1_output.log 2>&1 &

# æŸ¥çœ‹è¿›åº¦
tail -f exp1_output.log

# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
ps aux | grep exp1
```

---

## ğŸ“Š å®éªŒè¿è¡Œè¿‡ç¨‹

### ä½ ä¼šçœ‹åˆ°çš„è¾“å‡º

```
================================================================================
EXPERIMENT 1: Homogeneous Hardware Baseline
Model: qwen2.5-7b, Device: cuda:0
Number of verifiers per trial: 3
================================================================================

================================================================================
Trial 1: qwen2.5-7b on cuda:0
Prompt length: 245 chars
================================================================================

Loading model qwen2.5-7b to cuda:0...
Starting inference (Prefill + Decode)...
ç”Ÿæˆä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 856/1000 [02:07<00:00,  6.72it/s]

Inference complete: generated 856 tokens
Inference time: 127.34s

Starting verification 1/3 (Prefill only)...
Verification 1 complete: 0.98s

Starting verification 2/3 (Prefill only)...
Verification 2 complete: 1.02s

Starting verification 3/3 (Prefill only)...
Verification 3 complete: 0.95s

Verifier 1: Accept rate = 98.50%
Verifier 1: Overhead = 0.77%

Verifier 2: Accept rate = 98.52%
Verifier 2: Overhead = 0.80%

Verifier 3: Accept rate = 98.48%
Verifier 3: Overhead = 0.75%

Result saved to: ../data/raw/exp1/qwen2.5-7b_cuda:0_trial1.json
```

### æ—¶é—´ä¼°è®¡ï¼ˆå•ä¸ªA100ï¼‰

| é˜¶æ®µ | æ—¶é—´ | è¯´æ˜ |
|------|------|------|
| æ¨¡å‹åŠ è½½ | ~30ç§’ | é¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦5-10åˆ†é’Ÿ |
| æ¨ç†ï¼ˆPrefill+Decodeï¼‰ | ~2-3åˆ†é’Ÿ | ç”Ÿæˆ~800-1000 tokens |
| éªŒè¯ 3æ¬¡ | ~3ç§’ | æ¯æ¬¡çº¦1ç§’ï¼ˆä»…Prefillï¼‰ |
| å¯¹æ¯”åˆ†æ | ~5ç§’ | è®¡ç®—ç»Ÿè®¡é‡ |
| **å•ä¸ªtrialæ€»è®¡** | **~3-5åˆ†é’Ÿ** | |
| **3ä¸ªtrialsæ€»è®¡** | **~10-15åˆ†é’Ÿ** | |

---

## ğŸ“ å®éªŒç»“æœ

### è¾“å‡ºæ–‡ä»¶ä½ç½®

```
data/raw/exp1/
â”œâ”€â”€ qwen2.5-7b_cuda:0_trial1.json    # Trial 1ç»“æœ
â”œâ”€â”€ qwen2.5-7b_cuda:0_trial2.json    # Trial 2ç»“æœ
â”œâ”€â”€ qwen2.5-7b_cuda:0_trial3.json    # Trial 3ç»“æœ
â””â”€â”€ qwen2.5-7b_cuda:0_summary.json   # æ±‡æ€»ç»“æœï¼ˆæœ€é‡è¦ï¼‰
```

### æŸ¥çœ‹ç»“æœ

```bash
# æŸ¥çœ‹æ±‡æ€»ç»“æœ
cat data/raw/exp1/qwen2.5-7b_cuda:0_summary.json | jq '.aggregate'

# è¾“å‡ºç¤ºä¾‹ï¼š
# {
#   "avg_accept_rate": 0.9850,
#   "avg_overhead_percentage": 0.77,
#   "pass_rate": 1.0
# }

# æŸ¥çœ‹å•ä¸ªtrialçš„è¯¦ç»†ç»Ÿè®¡
cat data/raw/exp1/qwen2.5-7b_cuda:0_trial1.json | jq '.verifiers[0].statistics.summary'

# è¾“å‡ºç¤ºä¾‹ï¼š
# {
#   "accept_rate": 0.985,
#   "accept_count": 2891,
#   "total_count": 2934,
#   "avg_mean_error": 0.0089,
#   "avg_Pe": 0.0482
# }
```

### é¢„æœŸç»“æœï¼ˆå¯¹æ¯”è®ºæ–‡Table 3ï¼‰

| Metric | Expected (RTX 5090) | Your A100 | Status |
|--------|---------------------|-----------|--------|
| Accept Rate | > 95% | ~98.5% | âœ… |
| Overhead | ~0.8-1% | ~0.77% | âœ… |
| Mean Îµ | < 0.01 | ~0.009 | âœ… |
| Pe | < 0.05 | ~0.048 | âœ… |

---

## ğŸš€ å¤šGPUå¹¶è¡Œè¿è¡Œï¼ˆå¯é€‰ï¼‰

å¦‚æœæƒ³åˆ©ç”¨å…¨éƒ¨8å¼ A100å¹¶è¡Œè·‘ä¸åŒpromptsï¼š

### åˆ›å»ºå¹¶è¡Œè„šæœ¬ `run_exp1_parallel.sh`

```bash
#!/bin/bash
# Run 8 prompts in parallel on 8 A100s

for gpu_id in {0..7}; do
    echo "Starting on GPU $gpu_id..."
    CUDA_VISIBLE_DEVICES=$gpu_id python experiments/exp1_homogeneous.py \
        --model qwen2.5-7b \
        --device cuda:0 \
        --gpu-id $gpu_id \
        > logs/exp1_gpu${gpu_id}.log 2>&1 &
done

echo "All 8 GPUs started. Check progress:"
echo "  tail -f logs/exp1_gpu*.log"

wait
echo "All parallel jobs complete!"
```

**æ³¨æ„**ï¼šéœ€è¦ä¿®æ”¹ `exp1_homogeneous.py` æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜1: CUDA Out of Memory

```bash
# è§£å†³æ–¹æ¡ˆ1ï¼šå‡å°‘ç”Ÿæˆé•¿åº¦
# ç¼–è¾‘ experiments/exp1_homogeneous.py
# å°† max_new_tokens=1000 æ”¹ä¸º max_new_tokens=500

# è§£å†³æ–¹æ¡ˆ2ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹
# MODEL_NAME = "qwen2.5-3b"  # å¦‚æœæœ‰çš„è¯
```

### é—®é¢˜2: æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# æ£€æŸ¥ç½‘ç»œ
ping huggingface.co

# ä½¿ç”¨å›½å†…é•œåƒï¼ˆå¦‚æœåœ¨ä¸­å›½ï¼‰
export HF_ENDPOINT=https://hf-mirror.com
./run_exp1_a100.sh

# æˆ–æ‰‹åŠ¨è®¾ç½®é•œåƒ
export HF_DATASETS_OFFLINE=1
```

### é—®é¢˜3: å¯¼å…¥é”™è¯¯

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
pwd  # åº”è¯¥æ˜¾ç¤º .../verillm-experiments

# é‡æ–°å®‰è£…ä¾èµ–
pip install -r requirements.txt --force-reinstall
```

### é—®é¢˜4: æ‰¾ä¸åˆ°configsæ–‡ä»¶

```bash
# æ£€æŸ¥ç›®å½•ç»“æ„
ls configs/
# åº”è¯¥æœ‰: experiments.yaml  models.yaml  prompts.yaml

# å¦‚æœç¼ºå¤±ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®ç›®å½•
cd /path/to/verillm-experiments
```

---

## ğŸ“ˆ ä¸‹ä¸€æ­¥

å®éªŒ1æˆåŠŸåï¼Œä½ å¯ä»¥ï¼š

1. **åˆ†æç»“æœ**ï¼šå¯¹æ¯”ä½ çš„A100æ•°æ®ä¸è®ºæ–‡Table 3
2. **è°ƒæ•´å‚æ•°**ï¼šå°è¯•ä¸åŒçš„promptsã€æ¨¡å‹
3. **è¿è¡Œå®éªŒ2-5**ï¼šæŒ‰ç…§ [EXPERIMENT_GUIDE.md](EXPERIMENT_GUIDE.md)

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

é‡åˆ°é—®é¢˜å¯ä»¥ï¼š
1. æ£€æŸ¥ `logs/exp1_homogeneous_*.log` æ—¥å¿—æ–‡ä»¶
2. æŸ¥çœ‹ [EXPERIMENT_GUIDE.md](EXPERIMENT_GUIDE.md)
3. æŸ¥çœ‹ [QUICKSTART.md](QUICKSTART.md)
